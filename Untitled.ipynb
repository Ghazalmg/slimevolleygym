{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "score: -3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "\n",
    "env = gym.make(\"SlimeVolley-v0\")\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "  action = [1,0,1]\n",
    "  obs, reward, done, info = env.step(action,action)\n",
    "  total_reward += reward\n",
    "  env.render()\n",
    "env.close()\n",
    "\n",
    "print(\"score:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ale.lives': 0,\n",
       " 'ale.otherLives': 3,\n",
       " 'otherObs': array([ 0.2  ,  0.258,  0.   , -1.1  , -1.854,  0.222, -0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ]),\n",
       " 'state': array([ 0.2  ,  0.258,  0.   , -1.1  ,  1.854,  0.222,  0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ]),\n",
       " 'otherState': array([ 0.2  ,  0.258,  0.   , -1.1  , -1.854,  0.222, -0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0974,  0.1479, -0.0339,  0.0614,  0.0084,  0.0667,  0.1204,  0.0643],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(state).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_env = gym.make(env)\n",
    "state=env.reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2  ,  0.15 ,  0.   ,  0.   ,  0.   ,  1.2  , -0.35 ,  1.153,  1.2  ,  0.15 ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2000,  0.1500,  0.0000,  0.0000,  0.0000,  1.2000, -0.3500,  1.1528,\n",
       "         1.2000,  0.1500,  0.0000,  0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model.forward(torch.tensor(state).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(state, action1,action2, next_state, reward, done):\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y1 = reward + (1-done)*GAMMA*torch.max(target1(next_state))\n",
    "    model1_return = model1(state).squeeze()[action.long()].squeeze()\n",
    "    loss = loss_function(model1_return, y1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y2 = -reward + (1-done)*GAMMA*torch.max(target2(next_state))\n",
    "    model2_return = model2(state).squeeze()[action.long()].squeeze()\n",
    "    loss = loss_function(model2_return, y2)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f54b51ea807b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'action1' is not defined"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, _ = env.step(action1,action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(torch.tensor(2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model-path MODEL_PATH] [--env ENV]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Admin\\AppData\\Roaming\\jupyter\\runtime\\kernel-65509205-0480-4e76-9c1d-51a2e451bd42.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import slimevolleygym\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def binary(x, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
    "\n",
    "def eval_policy(policy1,policy2, env='SlimeVolley-v0', num_test_episodes=10, render=False, verbose=False):\n",
    "    test_env = gym.make(env)\n",
    "    test_rewards1 = []\n",
    "    test_rewards2 = []\n",
    "    for i in range(num_test_episodes):\n",
    "        state = test_env.reset() \n",
    "        episode_total_reward1 = 0\n",
    "        episode_total_reward2 = 0\n",
    "        while True:\n",
    "            state = torch.tensor([state], device=device, dtype=torch.float32)\n",
    "            action1 = binary(policy1.select_action(state).cpu().numpy()[0][0],3)\n",
    "            action2 = binary(policy2.select_action(state).cpu().numpy()[0][0],3)\n",
    "            next_state, reward, done, _ = test_env.step(action1,action2)\n",
    "            \n",
    "            if render:\n",
    "                test_env.render()\n",
    "            \n",
    "            episode_total_reward1 += reward\n",
    "            episode_total_reward2 -= reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                if verbose:\n",
    "                    print('[Episode {:4d}/{}] [reward {:.1f}][reward2 {:.1f}]'\n",
    "                        .format(i, num_test_episodes, episode_total_reward1,episode_total_reward2))\n",
    "                break\n",
    "        test_rewards1.append(episode_total_reward1)\n",
    "        test_rewards2.append(episode_total_reward2)\n",
    "    test_env.close()\n",
    "    return sum(test_rewards1)/num_test_episodes,sum(test_rewards2)/num_test_episodes\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    from model import MyModel\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-path', default=None, type=str,\n",
    "        help='Path to the model weights.')\n",
    "    parser.add_argument('--env', default=None, type=str,\n",
    "        help='Name of the environment.')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env = gym.make(args.env)\n",
    "    model1 = MyModel(state_size=len(env.reset()), action_size=8)\n",
    "    model1.load_state_dict(torch.load(args.model_path))\n",
    "    model1 = model.to(device)\n",
    "    model2 = MyModel(state_size=len(env.reset()), action_size=8)\n",
    "    model2.load_state_dict(torch.load(args.model_path))\n",
    "    model2 = model.to(device)\n",
    "    env.close()\n",
    "\n",
    "    eval_policy(policy1=model1,policy2=model2, env=args.env, render=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-4d9849ee1edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\model.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-4d9849ee1edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\model.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "model.select_action(torch.tensor(state).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.rand(1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=A.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(b,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2000,  0.1500,  0.0000,  0.0000,  0.0000,  1.2000, -1.1861,  1.7477,\n",
       "         1.2000,  0.1500,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env='SlimeVolley-v0'\n",
    "test_env = gym.make(env)\n",
    "test_rewards1 = []\n",
    "test_rewards2 = []\n",
    "state = test_env.reset() \n",
    "model1 = MyModel(state_size=len(test_env.reset()), action_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ce840135b17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mtarget1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mtarget1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mtarget1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "import torch\n",
    "from eval_policy import eval_policy, device\n",
    "from model import MyModel\n",
    "#from replay_buffer import ReplayBuffer\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import slimevolleygym\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "EPS_EXPLORATION = 0.2\n",
    "TARGET_UPDATE = 10\n",
    "NUM_EPISODES = 4000\n",
    "TEST_INTERVAL = 2\n",
    "LEARNING_RATE = 10e-4\n",
    "RENDER_INTERVAL = 20\n",
    "ENV_NAME = 'SlimeVolley-v0'\n",
    "PRINT_INTERVAL = 1\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "state_shape = len(env.reset())\n",
    "n_actions = 2**env.action_space.n\n",
    "\n",
    "model1 = MyModel(state_shape, n_actions).to(device)\n",
    "target1 = MyModel(state_shape, n_actions).to(device)\n",
    "target1.load_state_dict(model.state_dict())\n",
    "target1.eval()\n",
    "\n",
    "model2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2.load_state_dict(model.state_dict())\n",
    "target2.eval()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.MSELoss()\n",
    "# memory = ReplayBuffer()\n",
    "def binary(x, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
    "\n",
    "def choose_action(state, test_mode=False):\n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action1 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action1 = binary(torch.argmax(model1.forward(torch.tensor(state).float())),3)\n",
    "    \n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action2 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action2 = binary(torch.argmax(model2.forward(torch.tensor(state).float())),3)\n",
    "    return action1,action2\n",
    "   \n",
    "def optimize_model(state, action1,action2, next_state, reward, done):\n",
    "    state=torch.tensor(state).float()\n",
    "    next_state=torch.tensor(next_state).float()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y1 = reward + (1-done)*GAMMA*torch.max(target(next_state))\n",
    "    model1_return = model1(state).squeeze()[action1.long()].squeeze()\n",
    "    loss = loss_function(model1_return, y1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y2 = -reward + (1-done)*GAMMA*torch.max(target2(next_state))\n",
    "    model2_return = model2(state).squeeze()[action2.long()].squeeze()\n",
    "    loss = loss_function(model2_return, y2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "def train_reinforcement_learning(render=False):\n",
    "    steps_done = 0\n",
    "    best_score1 = -float(\"inf\")\n",
    "    best_score2 = -float(\"inf\")\n",
    "\n",
    "    for i_episode in range(1, NUM_EPISODES+1):\n",
    "        episode_total_reward1 = 0\n",
    "        episode_total_reward2 = 0\n",
    "        state = env.reset()\n",
    "        for t in count():\n",
    "            action1,action2 = choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action1,action2)\n",
    "            steps_done += 1\n",
    "            episode_total_reward1 += reward\n",
    "            episode_total_reward2 -= reward\n",
    "\n",
    "            optimize_model(state, action1, action2, next_state, reward, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                if i_episode % PRINT_INTERVAL == 0:\n",
    "                    print('[Episode {:4d}/{}] [Steps {:4d}] [reward1 {:.1f}][reward2 {:.1f}]'\n",
    "                        .format(i_episode, NUM_EPISODES, t, episode_total_reward1,episode_total_reward2))\n",
    "                break\n",
    "\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target.load_state_dict(model1.state_dict())\n",
    "            target2.load_state_dict(model2.state_dict())\n",
    "\n",
    "        if i_episode % TEST_INTERVAL == 0:\n",
    "            print('-'*10)\n",
    "            score1,score2 = eval_policy(policy1=model1,policy2=model2, env=ENV_NAME, render=render)\n",
    "            if score1 > best_score1:\n",
    "                best_score1 = score1\n",
    "                torch.save(model.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score1))\n",
    "            print('-'*10)\n",
    "            if score2 > best_score2:\n",
    "                best_score2 = score2\n",
    "                torch.save(model2.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score2))\n",
    "            print('-'*10)    \n",
    "\n",
    "\n",
    "train_reinforcement_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode    1/4000] [Steps  609] [reward1 -3.0][reward2 3.0]\n",
      "[Episode    2/4000] [Steps  640] [reward1 2.0][reward2 -2.0]\n",
      "----------\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b2d0b8e3c27a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m \u001b[0mtrain_reinforcement_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-b2d0b8e3c27a>\u001b[0m in \u001b[0;36mtrain_reinforcement_learning\u001b[1;34m(render)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mTEST_INTERVAL\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mscore1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicy2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mscore1\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \u001b[0mbest_score1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\eval_policy.py\u001b[0m in \u001b[0;36meval_policy\u001b[1;34m(policy1, policy2, env, num_test_episodes, render, verbose)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0maction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0maction2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\slimevolleygym\\slimevolley.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, otherAction)\u001b[0m\n\u001b[0;32m    785\u001b[0m       \u001b[0motherAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscreteToBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0motherAction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_left\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0motherAction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_right\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# external agent is agent_right\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\slimevolleygym\\slimevolley.py\u001b[0m in \u001b[0;36msetAction\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[0mbackward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0mjump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m       \u001b[0mforward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "import torch\n",
    "from eval_policy import eval_policy, device\n",
    "from model import MyModel\n",
    "#from replay_buffer import ReplayBuffer\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import slimevolleygym\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "EPS_EXPLORATION = 0.2\n",
    "TARGET_UPDATE = 10\n",
    "NUM_EPISODES = 4000\n",
    "TEST_INTERVAL = 2\n",
    "LEARNING_RATE = 10e-4\n",
    "RENDER_INTERVAL = 20\n",
    "ENV_NAME = 'SlimeVolley-v0'\n",
    "PRINT_INTERVAL = 1\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "state_shape = len(env.reset())\n",
    "n_actions = 2**env.action_space.n\n",
    "\n",
    "model1 = MyModel(state_shape, n_actions).to(device)\n",
    "target1 = MyModel(state_shape, n_actions).to(device)\n",
    "target1.load_state_dict(model1.state_dict())\n",
    "target1.eval()\n",
    "\n",
    "model2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2.load_state_dict(model2.state_dict())\n",
    "target2.eval()\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=LEARNING_RATE)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.MSELoss()\n",
    "# memory = ReplayBuffer()\n",
    "def binary(x, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
    "\n",
    "def choose_action(state, test_mode=False):\n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action1 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action1 = binary(torch.argmax(model1.forward(torch.tensor(state).float())),3)\n",
    "    \n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action2 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action2 = binary(torch.argmax(model2.forward(torch.tensor(state).float())),3)\n",
    "    return action1,action2\n",
    "   \n",
    "def optimize_model(state, action1,action2, next_state, reward, done):\n",
    "    state=torch.tensor(state).float()\n",
    "    next_state=torch.tensor(next_state).float()\n",
    "    optimizer1.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y1 = reward + (1-done)*GAMMA*torch.max(target1(next_state))\n",
    "    model1_return = model1(state).squeeze()[action1.long()].squeeze()\n",
    "    loss = loss_function(model1_return, y1)\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y2 = -reward + (1-done)*GAMMA*torch.max(target2(next_state))\n",
    "    model2_return = model2(state).squeeze()[action2.long()].squeeze()\n",
    "    loss = loss_function(model2_return, y2)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    \n",
    "def train_reinforcement_learning(render=False):\n",
    "    steps_done = 0\n",
    "    best_score1 = -float(\"inf\")\n",
    "    best_score2 = -float(\"inf\")\n",
    "\n",
    "    for i_episode in range(1, NUM_EPISODES+1):\n",
    "        episode_total_reward1 = 0\n",
    "        episode_total_reward2 = 0\n",
    "        state = env.reset()\n",
    "        for t in count():\n",
    "            action1,action2 = choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action1,action2)\n",
    "            steps_done += 1\n",
    "            episode_total_reward1 += reward\n",
    "            episode_total_reward2 -= reward\n",
    "\n",
    "            optimize_model(state, action1, action2, next_state, reward, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                if i_episode % PRINT_INTERVAL == 0:\n",
    "                    print('[Episode {:4d}/{}] [Steps {:4d}] [reward1 {:.1f}][reward2 {:.1f}]'\n",
    "                        .format(i_episode, NUM_EPISODES, t, episode_total_reward1,episode_total_reward2))\n",
    "                break\n",
    "\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target.load_state_dict(model1.state_dict())\n",
    "            target2.load_state_dict(model2.state_dict())\n",
    "\n",
    "        if i_episode % TEST_INTERVAL == 0:\n",
    "            print('-'*10)\n",
    "            score1,score2 = eval_policy(policy1=model1,policy2=model2, env=ENV_NAME, render=render)\n",
    "            if score1 > best_score1:\n",
    "                best_score1 = score1\n",
    "                torch.save(model.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score1))\n",
    "            print('-'*10)\n",
    "            if score2 > best_score2:\n",
    "                best_score2 = score2\n",
    "                torch.save(model2.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score2))\n",
    "            print('-'*10)    \n",
    "\n",
    "\n",
    "train_reinforcement_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
