{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "score: -3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "\n",
    "env = gym.make(\"SlimeVolley-v0\")\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "  action = [1,0,1]\n",
    "  obs, reward, done, info = env.step(action,action)\n",
    "  total_reward += reward\n",
    "  env.render()\n",
    "env.close()\n",
    "\n",
    "print(\"score:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ale.lives': 0,\n",
       " 'ale.otherLives': 3,\n",
       " 'otherObs': array([ 0.2  ,  0.258,  0.   , -1.1  , -1.854,  0.222, -0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ]),\n",
       " 'state': array([ 0.2  ,  0.258,  0.   , -1.1  ,  1.854,  0.222,  0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ]),\n",
       " 'otherState': array([ 0.2  ,  0.258,  0.   , -1.1  , -1.854,  0.222, -0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0974,  0.1479, -0.0339,  0.0614,  0.0084,  0.0667,  0.1204,  0.0643],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(state).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_env = gym.make(env)\n",
    "state=env.reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2  ,  0.15 ,  0.   ,  0.   ,  0.   ,  1.2  , -0.35 ,  1.153,  1.2  ,  0.15 ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2000,  0.1500,  0.0000,  0.0000,  0.0000,  1.2000, -0.3500,  1.1528,\n",
       "         1.2000,  0.1500,  0.0000,  0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model.forward(torch.tensor(state).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(state, action1,action2, next_state, reward, done):\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y1 = reward + (1-done)*GAMMA*torch.max(target1(next_state))\n",
    "    model1_return = model1(state).squeeze()[action.long()].squeeze()\n",
    "    loss = loss_function(model1_return, y1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y2 = -reward + (1-done)*GAMMA*torch.max(target2(next_state))\n",
    "    model2_return = model2(state).squeeze()[action.long()].squeeze()\n",
    "    loss = loss_function(model2_return, y2)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f54b51ea807b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'action1' is not defined"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, _ = env.step(action1,action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(torch.tensor(2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode    1/4000] [Steps  658] [reward1 2.0][reward2 -2.0]\n",
      "[Episode    2/4000] [Steps  679] [reward1 2.0][reward2 -2.0]\n",
      "----------\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "binary() missing 1 required positional argument: 'bits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-844539bd0af6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m \u001b[0mtrain_reinforcement_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-844539bd0af6>\u001b[0m in \u001b[0;36mtrain_reinforcement_learning\u001b[1;34m(render)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mTEST_INTERVAL\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mscore1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicy2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mscore1\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[0mbest_score1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\eval_policy.py\u001b[0m in \u001b[0;36meval_policy\u001b[1;34m(policy1, policy2, env, num_test_episodes, render, verbose)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0maction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0maction2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: binary() missing 1 required positional argument: 'bits'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "import torch\n",
    "from eval_policy import eval_policy, device\n",
    "from model import MyModel\n",
    "#from replay_buffer import ReplayBuffer\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import slimevolleygym\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "EPS_EXPLORATION = 0.2\n",
    "TARGET_UPDATE = 10\n",
    "NUM_EPISODES = 4000\n",
    "TEST_INTERVAL = 2\n",
    "LEARNING_RATE = 10e-4\n",
    "RENDER_INTERVAL = 20\n",
    "ENV_NAME = 'SlimeVolley-v0'\n",
    "PRINT_INTERVAL = 1\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "state_shape = len(env.reset())\n",
    "n_actions = 2**env.action_space.n\n",
    "\n",
    "model = MyModel(state_shape, n_actions).to(device)\n",
    "target = MyModel(state_shape, n_actions).to(device)\n",
    "target.load_state_dict(model.state_dict())\n",
    "target.eval()\n",
    "\n",
    "model2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2.load_state_dict(model.state_dict())\n",
    "target2.eval()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.MSELoss()\n",
    "# memory = ReplayBuffer()\n",
    "def binary(x, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
    "\n",
    "def choose_action(state, test_mode=False):\n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action1 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action1 = binary(torch.argmax(model.forward(torch.tensor(state).float())),3)\n",
    "    \n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action2 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action2 = binary(torch.argmax(model2.forward(torch.tensor(state).float())),3)\n",
    "    return action1,action2\n",
    "   \n",
    "def optimize_model(state, action1,action2, next_state, reward, done):\n",
    "    state=torch.tensor(state).float()\n",
    "    next_state=torch.tensor(next_state).float()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y1 = reward + (1-done)*GAMMA*torch.max(target(next_state))\n",
    "    model1_return = model(state).squeeze()[action1.long()].squeeze()\n",
    "    loss = loss_function(model1_return, y1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y2 = -reward + (1-done)*GAMMA*torch.max(target2(next_state))\n",
    "    model2_return = model2(state).squeeze()[action2.long()].squeeze()\n",
    "    loss = loss_function(model2_return, y2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "def train_reinforcement_learning(render=False):\n",
    "    steps_done = 0\n",
    "    best_score1 = -float(\"inf\")\n",
    "    best_score2 = -float(\"inf\")\n",
    "\n",
    "    for i_episode in range(1, NUM_EPISODES+1):\n",
    "        episode_total_reward1 = 0\n",
    "        episode_total_reward2 = 0\n",
    "        state = env.reset()\n",
    "        for t in count():\n",
    "            action1,action2 = choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action1,action2)\n",
    "            steps_done += 1\n",
    "            episode_total_reward1 += reward\n",
    "            episode_total_reward2 -= reward\n",
    "\n",
    "            optimize_model(state, action1, action2, next_state, reward, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                if i_episode % PRINT_INTERVAL == 0:\n",
    "                    print('[Episode {:4d}/{}] [Steps {:4d}] [reward1 {:.1f}][reward2 {:.1f}]'\n",
    "                        .format(i_episode, NUM_EPISODES, t, episode_total_reward1,episode_total_reward2))\n",
    "                break\n",
    "\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target.load_state_dict(model.state_dict())\n",
    "            target2.load_state_dict(model2.state_dict())\n",
    "\n",
    "        if i_episode % TEST_INTERVAL == 0:\n",
    "            print('-'*10)\n",
    "            score1,score2 = eval_policy(policy1=model,policy2=model2, env=ENV_NAME, render=render)\n",
    "            if score1 > best_score1:\n",
    "                best_score1 = score1\n",
    "                torch.save(model.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score1))\n",
    "            print('-'*10)\n",
    "            if score2 > best_score2:\n",
    "                best_score2 = score2\n",
    "                torch.save(model2.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score2))\n",
    "            print('-'*10)    \n",
    "\n",
    "\n",
    "train_reinforcement_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
