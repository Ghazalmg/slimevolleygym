{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "score: -3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import slimevolleygym\n",
    "\n",
    "env = gym.make(\"SlimeVolley-v0\")\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "  action = [1,0,1]\n",
    "  obs, reward, done, info = env.step(action,action)\n",
    "  total_reward += reward\n",
    "  env.render()\n",
    "env.close()\n",
    "\n",
    "print(\"score:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ale.lives': 0,\n",
       " 'ale.otherLives': 3,\n",
       " 'otherObs': array([ 0.2  ,  0.258,  0.   , -1.1  , -1.854,  0.222, -0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ]),\n",
       " 'state': array([ 0.2  ,  0.258,  0.   , -1.1  ,  1.854,  0.222,  0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ]),\n",
       " 'otherState': array([ 0.2  ,  0.258,  0.   , -1.1  , -1.854,  0.222, -0.832, -2.091,  0.2  ,  0.258,  0.   , -1.1  ])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0974,  0.1479, -0.0339,  0.0614,  0.0084,  0.0667,  0.1204,  0.0643],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(state).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_env = gym.make(env)\n",
    "state=env.reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2  ,  0.15 ,  0.   ,  0.   ,  0.   ,  1.2  , -0.35 ,  1.153,  1.2  ,  0.15 ,  0.   ,  0.   ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2000,  0.1500,  0.0000,  0.0000,  0.0000,  1.2000, -0.3500,  1.1528,\n",
       "         1.2000,  0.1500,  0.0000,  0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model.forward(torch.tensor(state).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(state, action1,action2, next_state, reward, done):\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y1 = reward + (1-done)*GAMMA*torch.max(target1(next_state))\n",
    "    model1_return = model1(state).squeeze()[action.long()].squeeze()\n",
    "    loss = loss_function(model1_return, y1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y2 = -reward + (1-done)*GAMMA*torch.max(target2(next_state))\n",
    "    model2_return = model2(state).squeeze()[action.long()].squeeze()\n",
    "    loss = loss_function(model2_return, y2)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f54b51ea807b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'action1' is not defined"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, _ = env.step(action1,action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(torch.tensor(2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model-path MODEL_PATH] [--env ENV]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Admin\\AppData\\Roaming\\jupyter\\runtime\\kernel-65509205-0480-4e76-9c1d-51a2e451bd42.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import slimevolleygym\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def binary(x, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
    "\n",
    "def eval_policy(policy1,policy2, env='SlimeVolley-v0', num_test_episodes=10, render=False, verbose=False):\n",
    "    test_env = gym.make(env)\n",
    "    test_rewards1 = []\n",
    "    test_rewards2 = []\n",
    "    for i in range(num_test_episodes):\n",
    "        state = test_env.reset() \n",
    "        episode_total_reward1 = 0\n",
    "        episode_total_reward2 = 0\n",
    "        while True:\n",
    "            state = torch.tensor([state], device=device, dtype=torch.float32)\n",
    "            action1 = binary(policy1.select_action(state).cpu().numpy()[0][0],3)\n",
    "            action2 = binary(policy2.select_action(state).cpu().numpy()[0][0],3)\n",
    "            next_state, reward, done, _ = test_env.step(action1,action2)\n",
    "            \n",
    "            if render:\n",
    "                test_env.render()\n",
    "            \n",
    "            episode_total_reward1 += reward\n",
    "            episode_total_reward2 -= reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                if verbose:\n",
    "                    print('[Episode {:4d}/{}] [reward {:.1f}][reward2 {:.1f}]'\n",
    "                        .format(i, num_test_episodes, episode_total_reward1,episode_total_reward2))\n",
    "                break\n",
    "        test_rewards1.append(episode_total_reward1)\n",
    "        test_rewards2.append(episode_total_reward2)\n",
    "    test_env.close()\n",
    "    return sum(test_rewards1)/num_test_episodes,sum(test_rewards2)/num_test_episodes\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    from model import MyModel\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-path', default=None, type=str,\n",
    "        help='Path to the model weights.')\n",
    "    parser.add_argument('--env', default=None, type=str,\n",
    "        help='Name of the environment.')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env = gym.make(args.env)\n",
    "    model1 = MyModel(state_size=len(env.reset()), action_size=8)\n",
    "    model1.load_state_dict(torch.load(args.model_path))\n",
    "    model1 = model.to(device)\n",
    "    model2 = MyModel(state_size=len(env.reset()), action_size=8)\n",
    "    model2.load_state_dict(torch.load(args.model_path))\n",
    "    model2 = model.to(device)\n",
    "    env.close()\n",
    "\n",
    "    eval_policy(policy1=model1,policy2=model2, env=args.env, render=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-4d9849ee1edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\model.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-4d9849ee1edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\project RL\\slimevolleygym\\model.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "model.select_action(torch.tensor(state).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.rand(1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=A.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(b,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2000,  0.1500,  0.0000,  0.0000,  0.0000,  1.2000, -1.1861,  1.7477,\n",
       "         1.2000,  0.1500,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env='SlimeVolley-v0'\n",
    "test_env = gym.make(env)\n",
    "test_rewards1 = []\n",
    "test_rewards2 = []\n",
    "state = test_env.reset() \n",
    "model1 = MyModel(state_size=len(test_env.reset()), action_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode    1/300] [Steps  524] [reward1 -5.0][reward2 5.0]\n",
      "[Episode    2/300] [Steps  628] [reward1 -5.0][reward2 5.0]\n",
      "[Episode    3/300] [Steps  621] [reward1 -5.0][reward2 5.0]\n",
      "[Episode    4/300] [Steps  475] [reward1 -5.0][reward2 5.0]\n",
      "[Episode    5/300] [Steps  727] [reward1 -4.0][reward2 4.0]\n",
      "[Episode    6/300] [Steps  513] [reward1 -5.0][reward2 5.0]\n",
      "[Episode    7/300] [Steps  543] [reward1 -5.0][reward2 5.0]\n",
      "[Episode    8/300] [Steps  419] [reward1 -5.0][reward2 5.0]\n",
      "[Episode    9/300] [Steps  512] [reward1 -5.0][reward2 5.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a94ad407362e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m \u001b[0mtrain_reinforcement_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a94ad407362e>\u001b[0m in \u001b[0;36mtrain_reinforcement_learning\u001b[1;34m(render)\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mplay_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0msteps_done\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a94ad407362e>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(state, test_mode)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0maction2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0maction2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a94ad407362e>\u001b[0m in \u001b[0;36mbinary\u001b[1;34m(x, bits)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# memory = ReplayBuffer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbits\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbitwise_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "import torch\n",
    "from eval_policy import eval_policy, device\n",
    "from model import MyModel\n",
    "#from replay_buffer import ReplayBuffer\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import slimevolleygym\n",
    "import simpleaudio as sa\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 1\n",
    "EPS_EXPLORATION = 0.2\n",
    "TARGET_UPDATE = 10\n",
    "NUM_EPISODES = 300\n",
    "TEST_INTERVAL = 5\n",
    "LEARNING_RATE = 10e-4\n",
    "RENDER_INTERVAL = 50\n",
    "ENV_NAME = 'SlimeVolley-v0'\n",
    "PRINT_INTERVAL = 1\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "state_shape = len(env.reset())\n",
    "n_actions = 2**env.action_space.n\n",
    "\n",
    "model1 = MyModel(state_shape, n_actions).to(device)\n",
    "target1 = MyModel(state_shape, n_actions).to(device)\n",
    "target1.load_state_dict(model1.state_dict())\n",
    "target1.eval()\n",
    "\n",
    "model2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2 = MyModel(state_shape, n_actions).to(device)\n",
    "target2.load_state_dict(model2.state_dict())\n",
    "target2.eval()\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=LEARNING_RATE)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.MSELoss()\n",
    "# memory = ReplayBuffer()\n",
    "def binary(x, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
    "\n",
    "def choose_action(state, test_mode=False):\n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action1 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action1 = binary(torch.argmax(model1.forward(torch.tensor(state).float())),3)\n",
    "    \n",
    "    r = np.random.random()\n",
    "    if r<EPS_EXPLORATION:\n",
    "        action2 = torch.tensor(env.action_space.sample())\n",
    "    else:\n",
    "        action2 = binary(torch.argmax(model2.forward(torch.tensor(state).float())),3)\n",
    "    return action1,action2\n",
    "   \n",
    "def optimize_model(state, action1,action2, next_state, reward, done):\n",
    "    state=torch.tensor(state).float()\n",
    "    next_state=torch.tensor(next_state).float()\n",
    "    optimizer1.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y1 = reward + (1-done)*GAMMA*torch.max(target1(next_state))\n",
    "    model1_return = model1(state).squeeze()[action1.long()].squeeze()\n",
    "    loss = loss_function(model1_return, y1)\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y2 = -reward + (1-done)*GAMMA*torch.max(target2(next_state))\n",
    "    model2_return = model2(state).squeeze()[action2.long()].squeeze()\n",
    "    loss = loss_function(model2_return, y2)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    \n",
    "def train_reinforcement_learning(render=False):\n",
    "    steps_done = 0\n",
    "    best_score1 = -float(\"inf\")\n",
    "    best_score2 = -float(\"inf\")\n",
    "\n",
    "    for i_episode in range(1, NUM_EPISODES+1):\n",
    "        episode_total_reward1 = 0\n",
    "        episode_total_reward2 = 0\n",
    "        state = env.reset()\n",
    "        if i_episode%RENDER_INTERVAL==0:\n",
    "            frequency = 600  # Our played note will be 440 Hz\n",
    "            fs = 44100  # 44100 samples per second\n",
    "            seconds = 1  # Note duration of 3 seconds\n",
    "\n",
    "\n",
    "            t = np.linspace(0, seconds, seconds * fs, False)\n",
    "\n",
    "            # Generate a 440 Hz sine wave\n",
    "            note = np.sin(frequency * t * 2 * np.pi)\n",
    "\n",
    "            # Ensure that highest value is in 16-bit range\n",
    "            audio = note * (2**15 - 1) / np.max(np.abs(note))\n",
    "            # Convert to 16-bit data\n",
    "            audio = audio.astype(np.int16)\n",
    "\n",
    "            # Start playback\n",
    "            play_obj = sa.play_buffer(audio, 1, 2, fs)\n",
    "\n",
    "            # Wait for playback to finish before exiting\n",
    "            play_obj.wait_done()\n",
    "        for t in count():\n",
    "            action1,action2 = choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action1)\n",
    "            steps_done += 1\n",
    "            episode_total_reward1 += reward\n",
    "            episode_total_reward2 -= reward\n",
    "\n",
    "            optimize_model(state, action1, action2, next_state, reward, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if i_episode%RENDER_INTERVAL == 0:\n",
    "                \n",
    "                env.render()\n",
    "                \n",
    "\n",
    "            if done:\n",
    "                if i_episode % PRINT_INTERVAL == 0:\n",
    "                    env.close()\n",
    "                    print('[Episode {:4d}/{}] [Steps {:4d}] [reward1 {:.1f}][reward2 {:.1f}]'\n",
    "                        .format(i_episode, NUM_EPISODES, t, episode_total_reward1,episode_total_reward2))\n",
    "                break\n",
    "\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target1.load_state_dict(model1.state_dict())\n",
    "            target2.load_state_dict(model2.state_dict())\n",
    "\n",
    "        if i_episode % TEST_INTERVAL == 0:\n",
    "            print('-'*10)\n",
    "            score1,score2,steppp = eval_policy(policy1=model1,policy2=model2, env=ENV_NAME, render=render)\n",
    "            if score1 > best_score1:\n",
    "                best_score1 = score1\n",
    "                torch.save(model1.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Steps {}]\".format(i_episode, steppp))\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score1))\n",
    "            print('-'*10)\n",
    "            if score2 > best_score2:\n",
    "                best_score2 = score2\n",
    "                torch.save(model2.state_dict(), \"best_model_{}.pt\".format(ENV_NAME))\n",
    "                print('saving model.')\n",
    "            print(\"[TEST Episode {}] [Average Reward {}]\".format(i_episode, score2))\n",
    "            print('-'*10)    \n",
    "\n",
    "\n",
    "train_reinforcement_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = 2 ** torch.arange(bits - 1, -1, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
